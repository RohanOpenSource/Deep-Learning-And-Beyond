{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ExplodingAndVanishingGradients.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNEGGHV820VPvfRlxqKS2fU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RohanOpenSource/Deep-Learning-And-Beyond/blob/main/ExplodingAndVanishingGradients.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d494qMPGjmfQ"
      },
      "source": [
        "Training neural networks can cause many problems that we can combat. The first of these is the exploding and vanishing gradient problem. During backpropogation, gradients can either become smaller and smaller, leaving to the lower layers of the neural network to remain unchanged, or they can become larger and larger making the neural network not work at all. This normally results in the usage of the sigmoid activation function or the normal distribution initlialization which both cause more variance between layers. To have this not happen, we need a similar variance for the inputs of each layer and the outputs. To acheive this, we can randomly initialize the weights. This is called Glorot and He Intilialization and is what is used by default in Keras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "814q-eMnje5q"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2wVWiaYoyP1"
      },
      "source": [
        "iris = load_iris() #you know the drill we're using iris\n",
        "X =  iris.data[:, 2:]\n",
        "y = iris.target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "krfcbMhwpLKd",
        "outputId": "4a8e729b-16aa-4409-e9ff-45f223981d1a"
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(128, activation=\"sigmoid\", kernel_initializer=tf.keras.initializers.GlorotUniform), #sigmoid can also cause exploding/vanishing gradients\n",
        "    tf.keras.layers.Dense(64, activation=\"relu\"), #relu isn't perfect either as it leads to some neurons always outputting 0, thus being useless\n",
        "    tf.keras.layers.Dense(32, activation=\"leaky_relu\"), # rather than making negative numbers 0, leaky relu squashes them quite heavily meaning no neurons become useless\n",
        "    tf.keras.layers.Dense(3, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "model.build([1, 2])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (1, 128)                  384       \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (1, 64)                   8256      \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (1, 32)                   2080      \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (1, 3)                    99        \n",
            "=================================================================\n",
            "Total params: 10,819\n",
            "Trainable params: 10,819\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvMUJY6Fbl_U",
        "outputId": "e5da677e-9f1e-4d2e-9ee3-dcac63634d30"
      },
      "source": [
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "model.fit(X, y, epochs=30)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "5/5 [==============================] - 1s 3ms/step - loss: 1.0754 - accuracy: 0.3667\n",
            "Epoch 2/30\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 1.0288 - accuracy: 0.5000\n",
            "Epoch 3/30\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.9890 - accuracy: 0.6667\n",
            "Epoch 4/30\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.9422 - accuracy: 0.8333\n",
            "Epoch 5/30\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.9018 - accuracy: 0.7800\n",
            "Epoch 6/30\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.8420 - accuracy: 0.9333\n",
            "Epoch 7/30\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.7749 - accuracy: 0.8133\n",
            "Epoch 8/30\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.7200 - accuracy: 0.7733\n",
            "Epoch 9/30\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.6567 - accuracy: 0.7400\n",
            "Epoch 10/30\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.6021 - accuracy: 0.7533\n",
            "Epoch 11/30\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.5496 - accuracy: 0.9067\n",
            "Epoch 12/30\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.5073 - accuracy: 0.9600\n",
            "Epoch 13/30\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.4630 - accuracy: 0.9600\n",
            "Epoch 14/30\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.4336 - accuracy: 0.8867\n",
            "Epoch 15/30\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.4095 - accuracy: 0.9533\n",
            "Epoch 16/30\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.3799 - accuracy: 0.9400\n",
            "Epoch 17/30\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.3555 - accuracy: 0.9400\n",
            "Epoch 18/30\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.3472 - accuracy: 0.9467\n",
            "Epoch 19/30\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.3322 - accuracy: 0.8933\n",
            "Epoch 20/30\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.3010 - accuracy: 0.9533\n",
            "Epoch 21/30\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.2905 - accuracy: 0.9467\n",
            "Epoch 22/30\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.2778 - accuracy: 0.9333\n",
            "Epoch 23/30\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.2600 - accuracy: 0.9600\n",
            "Epoch 24/30\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.2423 - accuracy: 0.9600\n",
            "Epoch 25/30\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.2317 - accuracy: 0.9600\n",
            "Epoch 26/30\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.2215 - accuracy: 0.9467\n",
            "Epoch 27/30\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.2097 - accuracy: 0.9733\n",
            "Epoch 28/30\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1995 - accuracy: 0.9667\n",
            "Epoch 29/30\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.1932 - accuracy: 0.9667\n",
            "Epoch 30/30\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.1834 - accuracy: 0.9667\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe6d8d16d10>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owAYWArLxcFq"
      },
      "source": [
        "Sometimes, using variants of relu and glorot initialization is not enough to prevent the gradients from vanishing or exploding. In this case, batch normalization is required. Batch Normalization normalizes and centers the inputs of the layer before it. This reduces the variance between layers and if used as the first layer, can remove the need to normalize the data before hand."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HyDhTPsHtbJC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd31beca-c66c-40ab-e068-37d1b95f9a5b"
      },
      "source": [
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()\n",
        "X_valid, X_train = X_train_full[:5000] /  255.0, X_train_full[5000:] / 255.0\n",
        "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
        "X_test = X_test / 255.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "40960/29515 [=========================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "26435584/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "16384/5148 [===============================================================================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n",
            "4431872/4422102 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXP7OJhtxrwu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6682a616-b088-4040-aa4d-1af66d15f9ae"
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten (Flatten)            (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 784)               3136      \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 300)               235500    \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 300)               1200      \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 10)                3010      \n",
            "=================================================================\n",
            "Total params: 242,846\n",
            "Trainable params: 240,678\n",
            "Non-trainable params: 2,168\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5NqTv9HwUxk0",
        "outputId": "d5671776-612e-4b36-dca4-4363332f54d6"
      },
      "source": [
        "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1719/1719 [==============================] - 13s 7ms/step - loss: 0.3742 - accuracy: 0.8625 - val_loss: 0.3315 - val_accuracy: 0.8758\n",
            "Epoch 2/10\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.3297 - accuracy: 0.8797 - val_loss: 0.3127 - val_accuracy: 0.8866\n",
            "Epoch 3/10\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.3020 - accuracy: 0.8873 - val_loss: 0.3118 - val_accuracy: 0.8922\n",
            "Epoch 4/10\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.2822 - accuracy: 0.8954 - val_loss: 0.3245 - val_accuracy: 0.8830\n",
            "Epoch 5/10\n",
            "1719/1719 [==============================] - 13s 7ms/step - loss: 0.2632 - accuracy: 0.9014 - val_loss: 0.3085 - val_accuracy: 0.8890\n",
            "Epoch 6/10\n",
            "1719/1719 [==============================] - 13s 7ms/step - loss: 0.2510 - accuracy: 0.9072 - val_loss: 0.3299 - val_accuracy: 0.8800\n",
            "Epoch 7/10\n",
            "1719/1719 [==============================] - 13s 7ms/step - loss: 0.2389 - accuracy: 0.9112 - val_loss: 0.3218 - val_accuracy: 0.8866\n",
            "Epoch 8/10\n",
            "1719/1719 [==============================] - 13s 8ms/step - loss: 0.2283 - accuracy: 0.9152 - val_loss: 0.3344 - val_accuracy: 0.8864\n",
            "Epoch 9/10\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.2167 - accuracy: 0.9195 - val_loss: 0.3308 - val_accuracy: 0.8936\n",
            "Epoch 10/10\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.2092 - accuracy: 0.9215 - val_loss: 0.3236 - val_accuracy: 0.8980\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe6d0a38bd0>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWuKHsxhXkI6"
      },
      "source": [
        "Another way to avoid exploding/vanishing gradients is to clip the gradients before they become to big or small during backpropogation. This is called gradient clipping. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ClO-o1KbX_GP",
        "outputId": "e7f51e69-5013-4343-99fe-485dbf18eca0"
      },
      "source": [
        "optimizer = tf.keras.optimizers.SGD(clipvalue=1.0)\n",
        "model.compile(loss = \"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
        "model.fit(X_train, y_train, epochs=5, validation_data=(X_valid, y_valid))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.1705 - accuracy: 0.9376 - val_loss: 0.2962 - val_accuracy: 0.9030\n",
            "Epoch 2/5\n",
            "1719/1719 [==============================] - 11s 6ms/step - loss: 0.1610 - accuracy: 0.9414 - val_loss: 0.2915 - val_accuracy: 0.9018\n",
            "Epoch 3/5\n",
            "1719/1719 [==============================] - 11s 6ms/step - loss: 0.1569 - accuracy: 0.9425 - val_loss: 0.2944 - val_accuracy: 0.9022\n",
            "Epoch 4/5\n",
            "1719/1719 [==============================] - 11s 6ms/step - loss: 0.1528 - accuracy: 0.9439 - val_loss: 0.2940 - val_accuracy: 0.9046\n",
            "Epoch 5/5\n",
            "1719/1719 [==============================] - 11s 6ms/step - loss: 0.1511 - accuracy: 0.9454 - val_loss: 0.2924 - val_accuracy: 0.9032\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe6d10cb610>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    }
  ]
}