{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DimensionalityReduction.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMgCOB52CeqdSD8sP8KJCop",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RohanOpenSource/ml-notebooks/blob/main/DimensionalityReduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lee9vGUpp834"
      },
      "source": [
        "The more dimensions we have in our data, the longer the model to take to train, the more extremes there will be, and the higher the likelyhood that our data will not be represented properly. This conecept in data science is reffered to as the **Curse Of Dimensionality**. The most popular way to do this is PCA, which is short for principle component analysis. In PCA we find the line that best represents the data, and then we find the line orthagonal to it, and the line orthagonal to that, until we run out of dimensions. Then, we use these lines to project the data into a lower dimension such that the easy to find pattern in the data is not lost."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mI4CrtoNsMNg"
      },
      "source": [
        "import numpy as np\n",
        "import sklearn"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tlw0Xbd2sajY"
      },
      "source": [
        "Let us start by making a random 3d dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kgxm_cL7rfj6"
      },
      "source": [
        "np.random.seed(42) \n",
        "m = 60\n",
        "w1, w2 = 0.1, 0.3\n",
        "noise = 0.1\n",
        "\n",
        "angles = np.random.rand(m) * 3 * np.pi / 2 - 0.5\n",
        "X = np.empty((m, 3))\n",
        "X[:, 0] = np.cos(angles) + np.sin(angles)/2 + noise * np.random.randn(m) / 2\n",
        "X[:, 1] = np.sin(angles) * 0.7 + noise * np.random.randn(m) / 2\n",
        "X[:, 2] = X[:, 0] * w1 + X[:, 1] * w2 + noise * np.random.randn(m)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14zXk2epsprR"
      },
      "source": [
        "Firstly, we will implement pca using numpy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72kmptZ4sWQB"
      },
      "source": [
        "X_centered = X - X.mean(axis=0) #centering the data at the origin\n",
        "U, S, VT = np.linalg.svd(X_centered)\n",
        "c1 = VT.T[:, 0]\n",
        "c2 = VT.T[:, 1]\n",
        "\n",
        "W2 = VT.T[:, :2]\n",
        "X2D = X_centered.dot(W2)#dot product of the data and matrix W"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AYbEJdduJhR"
      },
      "source": [
        "We can do this with much more ease using scikit learn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cGXHW8UvuUwR",
        "outputId": "91a26632-242b-490e-eafb-1be151d812bf"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "X2D = pca.fit_transform(X)\n",
        "pca.explained_variance_ratio_"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.85406025, 0.13622918])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQNP0i_Ku7XQ"
      },
      "source": [
        "To what dimension should be reduce the data to. 2 is a completely arbitrary number of dimensions and to few dimensions can be bad just as too many can. The easiest way to find the optimal number of dimensins is to add up the variance of a until it is sufficiently big."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybBLn--fwdb4"
      },
      "source": [
        "pca2 = PCA()#dont specify ndim\n",
        "pca2.fit_transform(X)\n",
        "cs = np.cumsum(pca.explained_variance_ratio_)\n",
        "d = np.argmax(cs >= 0.95)+1"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_ntbDPqw7QR"
      },
      "source": [
        "pca_o = PCA(n_components=d)\n",
        "X_r = pca_o.fit_transform(X)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kbpgx5iv5mGo"
      },
      "source": [
        "Just like batch gradient descent and stochastic gradient descent, calculating our principal components by sampling the entire dataset is quite slow. A quicker way to do it is sampling random data points"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6tOVe2axR_g"
      },
      "source": [
        "pca_rand = PCA(n_components=d, svd_solver=\"randomized\")\n",
        "X_reduced = pca_rand.fit_transform(X)"
      ],
      "execution_count": 33,
      "outputs": []
    }
  ]
}